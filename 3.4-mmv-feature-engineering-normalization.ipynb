{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix,load_npz,save_npz,hstack\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    NOTEBOOK_PATH=os.getcwd()\n",
    "    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)\n",
    "    return PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT=get_path()\n",
    "OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"train\")\n",
    "OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"test\")\n",
    "OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"cv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dictionary(path):\n",
    "    def vocab_builder(typ):\n",
    "        \"\"\"\n",
    "        Builds vocabulary for unigram and bigrams for byte chars\n",
    "        ````````````````````````````````````````````````````````\n",
    "        Args: unigram,bigram\n",
    "        N-gram type.\n",
    "        `````````````````````````````````\n",
    "        Returns: vocab: Unigram/Bigram\n",
    "        ```````````````````````\n",
    "        \"\"\"\n",
    "        hex_char=[hex(i).replace('0x','') for i in range(256)]\n",
    "        for i in range(len(hex_char)):\n",
    "            if len(hex_char[i])==1:\n",
    "                hex_char[i]='0'+hex_char[i]\n",
    "                \n",
    "        if typ=='bigram':\n",
    "            bi_vocab=[]\n",
    "            for first in hex_char:\n",
    "                for second in hex_char:\n",
    "                    bi_vocab.append(first+' '+second)\n",
    "            return bi_vocab  \n",
    "    \n",
    "        return hex_char\n",
    "    \n",
    "\n",
    "    feature_dictionary=[]\n",
    "    feature_dictionary.extend(('Byte_File_size','Asm_File_size'))\n",
    "    feature_dictionary.extend(vocab_builder('unigram'))\n",
    "    feature_dictionary.extend(vocab_builder('bigram'))\n",
    "    asm_images_list=[f'ASM_{num}' for num in range(1000)]\n",
    "    feature_dictionary.extend(asm_images_list)\n",
    "    return feature_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dictionary=get_feature_dictionary(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the saved files and convert to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='Byte_Asm_File_sizes.csv'\n",
    "\n",
    "file_sizes_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "file_sizes_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "file_sizes_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,file_name))\n",
    "\n",
    "file_sizes_train=file_sizes_train.drop([\"Id\"],axis=1)\n",
    "file_sizes_test=file_sizes_test.drop([\"Id\"],axis=1)\n",
    "file_sizes_cv=file_sizes_cv.drop([\"Id\"],axis=1)\n",
    "\n",
    "file_sizes_train=csr_matrix(file_sizes_train)\n",
    "file_sizes_test=csr_matrix(file_sizes_test)\n",
    "file_sizes_cv=csr_matrix(file_sizes_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='unigram_features_sparse.npz'\n",
    "\n",
    "unigram_features_train=load_npz(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "unigram_features_test=load_npz(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "unigram_features_cv=load_npz(os.path.join(OUTPUT_PATH_CV,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='bigram_features_sparse.npz'\n",
    "\n",
    "bigram_features_train=load_npz(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "bigram_features_test=load_npz(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "bigram_features_cv=load_npz(os.path.join(OUTPUT_PATH_CV,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='image_features_asm.csv'\n",
    "\n",
    "image_features_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "image_features_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "image_features_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,file_name))\n",
    "\n",
    "image_features_train=image_features_train.drop([\"ID\"],axis=1)\n",
    "image_features_test=image_features_test.drop([\"ID\"],axis=1)\n",
    "image_features_cv=image_features_cv.drop([\"ID\"],axis=1)\n",
    "\n",
    "image_features_train=csr_matrix(image_features_train)\n",
    "image_features_test=csr_matrix(image_features_test)\n",
    "image_features_cv=csr_matrix(image_features_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_train=hstack((file_sizes_train,\\\n",
    "                           unigram_features_train,\\\n",
    "                            bigram_features_train,\\\n",
    "                            image_features_train))\n",
    "\n",
    "all_features_test=hstack((file_sizes_test,\\\n",
    "                           unigram_features_test,\\\n",
    "                            bigram_features_test,\\\n",
    "                            image_features_test))\n",
    "\n",
    "all_features_cv=hstack((file_sizes_cv,\\\n",
    "                           unigram_features_cv,\\\n",
    "                            bigram_features_cv,\\\n",
    "                            image_features_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "X_train\n",
      "Feature normalization completed. Elapsed time:0.0 hours\n",
      "--------------------------------------------------\n",
      "X_test\n",
      "Feature normalization completed. Elapsed time:0.0 hours\n",
      "--------------------------------------------------\n",
      "X_cv\n",
      "Feature normalization completed. Elapsed time:0.0 hours\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "print(\"-\"*50)\n",
    "print(\"X_train\")\n",
    "start=time()\n",
    "all_features_train=all_features_train.toarray()\n",
    "all_features_train=scaler.fit_transform(all_features_train)\n",
    "all_features_train=csr_matrix(all_features_train)\n",
    "end=time()\n",
    "print(f\"Feature normalization completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(\"X_test\")\n",
    "start=time()\n",
    "all_features_test=all_features_test.toarray()\n",
    "all_features_test=scaler.fit_transform(all_features_test)\n",
    "all_features_test=csr_matrix(all_features_test)\n",
    "end=time()\n",
    "print(f\"Feature normalization completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "print(\"-\"*50) \n",
    "\n",
    "\n",
    "print(\"X_cv\")\n",
    "start=time()\n",
    "all_features_cv=all_features_cv.toarray()\n",
    "all_features_cv=scaler.fit_transform(all_features_cv)\n",
    "all_features_cv=csr_matrix(all_features_cv)\n",
    "end=time()\n",
    "print(f\"Feature normalization completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the normalized featues to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(os.path.join(OUTPUT_PATH_TRAIN,'all_features_normalized.npz'),all_features_train)\n",
    "save_npz(os.path.join(OUTPUT_PATH_TEST,'all_features_normalized.npz'),all_features_test)\n",
    "save_npz(os.path.join(OUTPUT_PATH_CV,'all_features_normalized.npz'),all_features_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_feature_dictionary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"get_feature_dictionary.py\"\n",
    "def get_feature_dictionary(path):\n",
    "    def vocab_builder(typ):\n",
    "        \"\"\"\n",
    "        Builds vocabulary for unigram and bigrams for byte chars\n",
    "        ````````````````````````````````````````````````````````\n",
    "        Args: unigram,bigram\n",
    "        N-gram type.\n",
    "        `````````````````````````````````\n",
    "        Returns: vocab: Unigram/Bigram\n",
    "        ```````````````````````\n",
    "        \"\"\"\n",
    "        hex_char=[hex(i).replace('0x','') for i in range(256)]\n",
    "        for i in range(len(hex_char)):\n",
    "            if len(hex_char[i])==1:\n",
    "                hex_char[i]='0'+hex_char[i]\n",
    "                \n",
    "        if typ=='bigram':\n",
    "            bi_vocab=[]\n",
    "            for first in hex_char:\n",
    "                for second in hex_char:\n",
    "                    bi_vocab.append(first+' '+second)\n",
    "            return bi_vocab  \n",
    "    \n",
    "        return hex_char\n",
    "    \n",
    "\n",
    "    feature_dictionary=[]\n",
    "    feature_dictionary.extend(('Byte_File_size','Asm_File_size'))\n",
    "    feature_dictionary.extend(vocab_builder('unigram'))\n",
    "    feature_dictionary.extend(vocab_builder('bigram'))\n",
    "    asm_images_list=[f'ASM_{num}' for num in range(1000)]\n",
    "    feature_dictionary.extend(asm_images_list)\n",
    "    return feature_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting F:\\\\Microsoft Malware Detection\\\\src\\\\features\\\\get_feature_dictionary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"F:\\\\Microsoft Malware Detection\\\\src\\\\features\\\\get_feature_dictionary.py\"\n",
    "def get_feature_dictionary(path):\n",
    "    def vocab_builder(typ):\n",
    "        \"\"\"\n",
    "        Builds vocabulary for unigram and bigrams for byte chars\n",
    "        ````````````````````````````````````````````````````````\n",
    "        Args: unigram,bigram\n",
    "        N-gram type.\n",
    "        `````````````````````````````````\n",
    "        Returns: vocab: Unigram/Bigram\n",
    "        ```````````````````````\n",
    "        \"\"\"\n",
    "        hex_char=[hex(i).replace('0x','') for i in range(256)]\n",
    "        for i in range(len(hex_char)):\n",
    "            if len(hex_char[i])==1:\n",
    "                hex_char[i]='0'+hex_char[i]\n",
    "                \n",
    "        if typ=='bigram':\n",
    "            bi_vocab=[]\n",
    "            for first in hex_char:\n",
    "                for second in hex_char:\n",
    "                    bi_vocab.append(first+' '+second)\n",
    "            return bi_vocab  \n",
    "    \n",
    "        return hex_char\n",
    "    \n",
    "\n",
    "    feature_dictionary=[]\n",
    "    feature_dictionary.extend(('Byte_File_size','Asm_File_size'))\n",
    "    feature_dictionary.extend(vocab_builder('unigram'))\n",
    "    feature_dictionary.extend(vocab_builder('bigram'))\n",
    "    asm_images_list=[f'ASM_{num}' for num in range(1000)]\n",
    "    feature_dictionary.extend(asm_images_list)\n",
    "    return feature_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting F:\\\\Microsoft Malware Detection\\\\src\\\\features\\\\3.4-mmv-feature-engineering-normalization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"F:\\\\Microsoft Malware Detection\\\\src\\\\features\\\\3.4-mmv-feature-engineering-normalization.py\"\n",
    "# Data Normalization\n",
    "## Import necessary libraries\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix,load_npz,save_npz,hstack\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "## Constants\n",
    "def get_path():\n",
    "    NOTEBOOK_PATH=os.getcwd()\n",
    "    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)\n",
    "    return PROJECT_ROOT\n",
    "PROJECT_ROOT=get_path()\n",
    "OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"train\")\n",
    "OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"test\")\n",
    "OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"cv\")\n",
    "\n",
    "\n",
    "## Read the saved files and convert to sparse matrix\n",
    "file_name='Byte_Asm_File_sizes.csv'\n",
    "\n",
    "file_sizes_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "file_sizes_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "file_sizes_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,file_name))\n",
    "\n",
    "file_sizes_train=file_sizes_train.drop([\"Id\"],axis=1)\n",
    "file_sizes_test=file_sizes_test.drop([\"Id\"],axis=1)\n",
    "file_sizes_cv=file_sizes_cv.drop([\"Id\"],axis=1)\n",
    "\n",
    "\n",
    "file_sizes_train=csr_matrix(file_sizes_train)\n",
    "file_sizes_test=csr_matrix(file_sizes_test)\n",
    "file_sizes_cv=csr_matrix(file_sizes_cv)\n",
    "\n",
    "file_name='unigram_features_sparse.npz'\n",
    "\n",
    "unigram_features_train=load_npz(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "unigram_features_test=load_npz(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "unigram_features_cv=load_npz(os.path.join(OUTPUT_PATH_CV,file_name))\n",
    "\n",
    "file_name='bigram_features_sparse.npz'\n",
    "\n",
    "bigram_features_train=load_npz(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "bigram_features_test=load_npz(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "bigram_features_cv=load_npz(os.path.join(OUTPUT_PATH_CV,file_name))\n",
    "\n",
    "file_name='image_features_asm.csv'\n",
    "\n",
    "image_features_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,file_name))\n",
    "image_features_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,file_name))\n",
    "image_features_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,file_name))\n",
    "\n",
    "image_features_train=image_features_train.drop([\"ID\"],axis=1)\n",
    "image_features_test=image_features_test.drop([\"ID\"],axis=1)\n",
    "image_features_cv=image_features_cv.drop([\"ID\"],axis=1)\n",
    "\n",
    "\n",
    "image_features_train=csr_matrix(image_features_train)\n",
    "image_features_test=csr_matrix(image_features_test)\n",
    "image_features_cv=csr_matrix(image_features_cv)\n",
    "## Concatenate all features\n",
    "all_features_train=hstack((file_sizes_train,\\\n",
    "                           unigram_features_train,\\\n",
    "                            bigram_features_train,\\\n",
    "                            image_features_train))\n",
    "\n",
    "all_features_test=hstack((file_sizes_test,\\\n",
    "                           unigram_features_test,\\\n",
    "                            bigram_features_test,\\\n",
    "                            image_features_test))\n",
    "\n",
    "all_features_cv=hstack((file_sizes_cv,\\\n",
    "                           unigram_features_cv,\\\n",
    "                            bigram_features_cv,\\\n",
    "                            image_features_cv))\n",
    "## Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "print(\"-\"*50)\n",
    "print(\"X_train\")\n",
    "start=time()\n",
    "all_features_train=all_features_train.toarray()\n",
    "all_features_train=scaler.fit_transform(all_features_train)\n",
    "all_features_train=csr_matrix(all_features_train)\n",
    "end=time()\n",
    "print(f\"Feature normalization completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(\"X_test\")\n",
    "start=time()\n",
    "all_features_test=all_features_test.toarray()\n",
    "all_features_test=scaler.fit_transform(all_features_test)\n",
    "all_features_test=csr_matrix(all_features_test)\n",
    "end=time()\n",
    "print(f\"Feature normalization completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "print(\"-\"*50) \n",
    "\n",
    "\n",
    "print(\"X_cv\")\n",
    "start=time()\n",
    "all_features_cv=all_features_cv.toarray()\n",
    "all_features_cv=scaler.fit_transform(all_features_cv)\n",
    "all_features_cv=csr_matrix(all_features_cv)\n",
    "end=time()\n",
    "print(f\"Feature normalization completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "## Save the normalized featues to file\n",
    "save_npz(os.path.join(OUTPUT_PATH_TRAIN,'all_features_normalized.npz'),all_features_train)\n",
    "save_npz(os.path.join(OUTPUT_PATH_TEST,'all_features_normalized.npz'),all_features_test)\n",
    "save_npz(os.path.join(OUTPUT_PATH_CV,'all_features_normalized.npz'),all_features_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_custom(col):\n",
    "    '''\n",
    "    min_max_custom(unigram_features_cv[:,100].toarray())\n",
    "    '''\n",
    "    return MinMaxScaler().fit_transform(col).round(4)\n",
    "\n",
    "def normalizer(x):\n",
    "    arr=x.copy()\n",
    "    col_len=arr.shape[1]\n",
    "    for col_idx in tqdm(range(col_len)):\n",
    "        arr[:,col_idx]= min_max_custom(arr[:,col_idx].toarray())\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.49632599e-01, 2.36700971e-01, 2.83019197e-03, ...,\n",
       "        4.11111111e-01, 5.33333333e-01, 4.09090909e-01],\n",
       "       [5.56668893e-03, 6.88505581e-03, 1.12426123e-02, ...,\n",
       "        2.22222222e-01, 3.71428571e-01, 1.93181818e-01],\n",
       "       [3.38454687e-02, 3.68894043e-02, 3.06029243e-03, ...,\n",
       "        0.00000000e+00, 2.19047619e-01, 2.95454545e-01],\n",
       "       ...,\n",
       "       [9.35203741e-03, 2.22496014e-02, 8.78878961e-03, ...,\n",
       "        9.22222222e-01, 8.76190476e-01, 9.54545455e-01],\n",
       "       [1.49855266e-01, 5.79794173e-04, 4.95654725e-03, ...,\n",
       "        9.33333333e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.35203741e-03, 4.71082766e-03, 1.85783197e-02, ...,\n",
       "        4.11111111e-01, 5.33333333e-01, 4.09090909e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=all_features_train.toarray()\n",
    "d=MinMaxScaler().fit_transform(d)\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
