{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering for unigram of byte files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix,vstack,save_npz,load_npz\n",
    "from time import time\n",
    "from joblib import Parallel,delayed\n",
    "# from train_test_cv_split import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-cv split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    NOTEBOOK_PATH=os.getcwd()\n",
    "    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)\n",
    "    return PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT=get_path()\n",
    "OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"train\")\n",
    "OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"test\")\n",
    "OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"cv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,\"X_train.csv\"))\n",
    "X_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,\"X_test.csv\"))\n",
    "X_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,\"X_cv.csv\"))\n",
    "y_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,\"y_train.csv\"))\n",
    "y_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,\"y_test.csv\"))\n",
    "y_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,\"y_cv.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    NOTEBOOK_PATH=os.getcwd()\n",
    "    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)\n",
    "    return PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT=get_path()\n",
    "BYTE_FILES_PATH=os.path.join(PROJECT_ROOT,\"data\",\"interim\",\"byteFiles\")\n",
    "OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"train\")\n",
    "OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"test\")\n",
    "OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"cv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from byte files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_splitter(files):\n",
    "    '''\n",
    "    Processing list of files for parallel processing. \n",
    "    Configured for machine with 10 vCPU. \n",
    "    Can be hard coded to attain high throughput\n",
    "    ``````````````````````````````````````````````````````````````````````````````````\n",
    "    Args: files: list of filepaths\n",
    "    ```````````````````````\n",
    "    Returns: Filepaths splitted equally for each process\n",
    "    ````````````````````````````````````````````````````\n",
    "    '''\n",
    "    n_cpu=10 #Modify if needed\n",
    "    files_len=len(files)\n",
    "    partition_files=int(files_len/n_cpu) #hard code if needed\n",
    "\n",
    "    files_splitted=[files[x:x+partition_files] for x in range(0,files_len,partition_files)] \n",
    "    return files_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10868/10868 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#removal of addres from byte files\n",
    "# ----------------\n",
    "# contents of .byte files\n",
    "#00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08 \n",
    "#-------------------\n",
    "#we remove the starting address 00401000\n",
    "\n",
    "files = os.listdir(BYTE_FILES_PATH)\n",
    "filenames=[]\n",
    "array=[]\n",
    "for file in tqdm(files):\n",
    "    if(file.endswith(\"bytes\")):\n",
    "        file=file.split('.')[0]\n",
    "        text_file = open(os.path.join(BYTE_FILES_PATH,f\"{file}.txt\"), 'w+')\n",
    "        with open(os.path.join(BYTE_FILES_PATH,f\"{file}.bytes\"),\"r\") as fp:\n",
    "            lines=\"\"\n",
    "            for line in fp:\n",
    "                a=line.rstrip().split(\" \")[1:]\n",
    "                b=' '.join(a)\n",
    "                b=b+\"\\n\"\n",
    "                text_file.write(b)\n",
    "            fp.close()\n",
    "            os.remove(os.path.join(BYTE_FILES_PATH,f\"{file}.bytes\"))\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_builder(typ):\n",
    "    \"\"\"\n",
    "    Builds vocabulary for unigram and bigrams for byte chars\n",
    "    ````````````````````````````````````````````````````````\n",
    "    Args: unigram,bigram\n",
    "    N-gram type.\n",
    "    `````````````````````````````````\n",
    "    Returns: vocab: Unigram/Bigram\n",
    "    ```````````````````````\n",
    "    \"\"\"\n",
    "    hex_char=[hex(i).replace('0x','') for i in range(256)]\n",
    "    for i in range(len(hex_char)):\n",
    "        if len(hex_char[i])==1:\n",
    "            hex_char[i]='0'+hex_char[i]\n",
    "            \n",
    "    if typ=='bigram':\n",
    "        bi_vocab=[]\n",
    "        for first in hex_char:\n",
    "            for second in hex_char:\n",
    "                bi_vocab.append(first+' '+second)\n",
    "        return bi_vocab  \n",
    "    \n",
    "    return hex_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bytes_unigram_extraction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bytes_unigram_extraction.py\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix,vstack\n",
    "\n",
    "def bytes_unigram_extraction(files,vocab):\n",
    "    vocab_len=len(vocab)\n",
    "    total_byte_files_len=len(files)\n",
    "    vectorizer=CountVectorizer(ngram_range=(1,1))\n",
    "    vectorizer.fit_transform(vocab)\n",
    "    unigram=[]\n",
    "    for file_name_idx in tqdm(range(total_byte_files_len)):\n",
    "        with open(files[file_name_idx]+'.txt','r') as temp:\n",
    "            all_lines=temp.read().replace(\"\\n\",\" \").replace(\"  \",\" \").lower()\n",
    "            unigram_temp=vectorizer.transform([all_lines])\n",
    "            unigram.append(unigram_temp)\n",
    "    unigram=vstack(unigram)\n",
    "    return unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bytes_unigram_extraction import bytes_unigram_extraction\n",
    "def get_unigram_feature_v2(files,path_to_file):\n",
    "    path_to_file=os.path.join(path_to_file,\"unigram_features_sparse.npz\")\n",
    "    if not os.path.isfile(path_to_file):\n",
    "        start=time()\n",
    "        print(\"Extracting unigram features\")\n",
    "        files_splitted=files_splitter(files)\n",
    "        vocab_uni=vocab_builder('unigram')\n",
    "        \n",
    "        list_of_unigrams=Parallel(n_jobs=-1)\\\n",
    "            (delayed (bytes_unigram_extraction)(files_splitted_,vocab_uni) for files_splitted_ in files_splitted)\n",
    "        unigrams=vstack(list_of_unigrams)\n",
    "        save_npz(path_to_file,unigrams,compressed=True)\n",
    "        end=time()\n",
    "        print(f\"Extracting unigram features completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(f\"{path_to_file} is already present!\")\n",
    "        unigrams=load_npz(path_to_file)\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "X_train\n",
      "Extracting unigram features\n",
      "Extracting unigram features completed. Elapsed time:0.08 hours\n",
      "--------------------------------------------------\n",
      "X_test\n",
      "Extracting unigram features\n",
      "Extracting unigram features completed. Elapsed time:0.03 hours\n",
      "--------------------------------------------------\n",
      "X_cv\n",
      "Extracting unigram features\n",
      "Extracting unigram features completed. Elapsed time:0.03 hours\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filenames_train=[f\"{file_name}\" for file_name in X_train.Id.values]\n",
    "filenames_test=[f\"{file_name}\" for file_name in X_test.Id.values]\n",
    "filenames_cv=[f\"{file_name}\" for file_name in X_cv.Id.values]\n",
    "\n",
    "file_ID_train=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_train]\n",
    "file_ID_test=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_test]\n",
    "file_ID_cv=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_cv]\n",
    "\n",
    "uni_gram_features=vocab_builder('unigram')\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"X_train\")\n",
    "unigrams_train=get_unigram_feature_v2(file_ID_train,OUTPUT_PATH_TRAIN)\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(\"X_test\")\n",
    "unigrams_test=get_unigram_feature_v2(file_ID_test,OUTPUT_PATH_TEST)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"X_cv\")\n",
    "unigrams_cv=get_unigram_feature_v2(file_ID_cv,OUTPUT_PATH_CV)\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting F:\\Microsoft Malware Detection\\src\\features\\bytes_unigram_extraction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"F:\\Microsoft Malware Detection\\src\\features\\bytes_unigram_extraction.py\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix,vstack\n",
    "\n",
    "def bytes_unigram_extraction(files,vocab):\n",
    "    vocab_len=len(vocab)\n",
    "    total_byte_files_len=len(files)\n",
    "    vectorizer=CountVectorizer(ngram_range=(1,1))\n",
    "    vectorizer.fit_transform(vocab)\n",
    "    unigram=[]\n",
    "    for file_name_idx in tqdm(range(total_byte_files_len)):\n",
    "        with open(files[file_name_idx]+'.txt','r') as temp:\n",
    "            all_lines=temp.read().replace(\"\\n\",\" \").replace(\"  \",\" \").lower()\n",
    "            unigram_temp=vectorizer.transform([all_lines])\n",
    "            unigram.append(unigram_temp)\n",
    "    unigram=vstack(unigram)\n",
    "    return unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting F:\\Microsoft Malware Detection\\src\\features\\3.1-mmv-feature_eng_unigram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"F:\\Microsoft Malware Detection\\src\\features\\3.1-mmv-feature_eng_unigram.py\"\n",
    "# Feature engineering for unigram of byte files\n",
    "## Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix,vstack,save_npz,load_npz\n",
    "from time import time\n",
    "from joblib import Parallel,delayed\n",
    "# from train_test_cv_split import read_data\n",
    "## Train-test-cv split\n",
    "def get_path():\n",
    "    NOTEBOOK_PATH=os.getcwd()\n",
    "    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)\n",
    "    return PROJECT_ROOT\n",
    "PROJECT_ROOT=get_path()\n",
    "OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"train\")\n",
    "OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"test\")\n",
    "OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"cv\")\n",
    "\n",
    "\n",
    "X_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,\"X_train.csv\"))\n",
    "X_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,\"X_test.csv\"))\n",
    "X_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,\"X_cv.csv\"))\n",
    "y_train=pd.read_csv(os.path.join(OUTPUT_PATH_TRAIN,\"y_train.csv\"))\n",
    "y_test=pd.read_csv(os.path.join(OUTPUT_PATH_TEST,\"y_test.csv\"))\n",
    "y_cv=pd.read_csv(os.path.join(OUTPUT_PATH_CV,\"y_cv.csv\"))## Constants\n",
    "def get_path():\n",
    "    NOTEBOOK_PATH=os.getcwd()\n",
    "    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)\n",
    "    return PROJECT_ROOT\n",
    "\n",
    "PROJECT_ROOT=get_path()\n",
    "BYTE_FILES_PATH=os.path.join(PROJECT_ROOT,\"data\",\"interim\",\"byteFiles\")\n",
    "OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"train\")\n",
    "OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"test\")\n",
    "OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,\"data\",\"processed\",\"cv\")\n",
    "\n",
    "\n",
    "## Feature extraction from byte files \n",
    "def files_splitter(files):\n",
    "    '''\n",
    "    Processing list of files for parallel processing. \n",
    "    Configured for machine with 10 vCPU. \n",
    "    Can be hard coded to attain high throughput\n",
    "    ``````````````````````````````````````````````````````````````````````````````````\n",
    "    Args: files: list of filepaths\n",
    "    ```````````````````````\n",
    "    Returns: Filepaths splitted equally for each process\n",
    "    ````````````````````````````````````````````````````\n",
    "    '''\n",
    "    n_cpu=10 #Modify if needed\n",
    "    files_len=len(files)\n",
    "    partition_files=int(files_len/n_cpu) #hard code if needed\n",
    "\n",
    "    files_splitted=[files[x:x+partition_files] for x in range(0,files_len,partition_files)] \n",
    "    return files_splitted\n",
    "\n",
    "#removal of addres from byte files\n",
    "# ----------------\n",
    "# contents of .byte files\n",
    "#00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08 \n",
    "#-------------------\n",
    "#we remove the starting address 00401000\n",
    "\n",
    "files = os.listdir(BYTE_FILES_PATH)\n",
    "filenames=[]\n",
    "array=[]\n",
    "for file in tqdm(files):\n",
    "    if(file.endswith(\"bytes\")):\n",
    "        file=file.split('.')[0]\n",
    "        text_file = open(os.path.join(BYTE_FILES_PATH,f\"{file}.txt\"), 'w+')\n",
    "        with open(os.path.join(BYTE_FILES_PATH,f\"{file}.bytes\"),\"r\") as fp:\n",
    "            lines=\"\"\n",
    "            for line in fp:\n",
    "                a=line.rstrip().split(\" \")[1:]\n",
    "                b=' '.join(a)\n",
    "                b=b+\"\\n\"\n",
    "                text_file.write(b)\n",
    "            fp.close()\n",
    "            os.remove(os.path.join(BYTE_FILES_PATH,f\"{file}.bytes\"))\n",
    "        text_file.close()\n",
    "\n",
    "\n",
    "def vocab_builder(typ):\n",
    "    \"\"\"\n",
    "    Builds vocabulary for unigram and bigrams for byte chars\n",
    "    ````````````````````````````````````````````````````````\n",
    "    Args: unigram,bigram\n",
    "    N-gram type.\n",
    "    `````````````````````````````````\n",
    "    Returns: vocab: Unigram/Bigram\n",
    "    ```````````````````````\n",
    "    \"\"\"\n",
    "    hex_char=[hex(i).replace('0x','') for i in range(256)]\n",
    "    for i in range(len(hex_char)):\n",
    "        if len(hex_char[i])==1:\n",
    "            hex_char[i]='0'+hex_char[i]\n",
    "            \n",
    "    if typ=='bigram':\n",
    "        bi_vocab=[]\n",
    "        for first in hex_char:\n",
    "            for second in hex_char:\n",
    "                bi_vocab.append(first+' '+second)\n",
    "        return bi_vocab  \n",
    "    \n",
    "    return hex_char\n",
    "\n",
    "from bytes_unigram_extraction import bytes_unigram_extraction\n",
    "def get_unigram_feature_v2(files,path_to_file):\n",
    "    path_to_file=os.path.join(path_to_file,\"unigram_features_sparse.npz\")\n",
    "    if not os.path.isfile(path_to_file):\n",
    "        start=time()\n",
    "        print(\"Extracting unigram features\")\n",
    "        files_splitted=files_splitter(files)\n",
    "        vocab_uni=vocab_builder('unigram')\n",
    "        \n",
    "        list_of_unigrams=Parallel(n_jobs=-1)\\\n",
    "            (delayed (bytes_unigram_extraction)(files_splitted_,vocab_uni) for files_splitted_ in files_splitted)\n",
    "        unigrams=vstack(list_of_unigrams)\n",
    "        save_npz(path_to_file,unigrams,compressed=True)\n",
    "        end=time()\n",
    "        print(f\"Extracting unigram features completed. Elapsed time:{round((end-start)/3600,2)} hours\")\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(f\"{path_to_file} is already present!\")\n",
    "        unigrams=load_npz(path_to_file)\n",
    "    return unigrams\n",
    "\n",
    "\n",
    "filenames_train=[f\"{file_name}\" for file_name in X_train.Id.values]\n",
    "filenames_test=[f\"{file_name}\" for file_name in X_test.Id.values]\n",
    "filenames_cv=[f\"{file_name}\" for file_name in X_cv.Id.values]\n",
    "\n",
    "file_ID_train=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_train]\n",
    "file_ID_test=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_test]\n",
    "file_ID_cv=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_cv]\n",
    "\n",
    "uni_gram_features=vocab_builder('unigram')\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"X_train\")\n",
    "unigrams_train=get_unigram_feature_v2(file_ID_train,OUTPUT_PATH_TRAIN)\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(\"X_test\")\n",
    "unigrams_test=get_unigram_feature_v2(file_ID_test,OUTPUT_PATH_TEST)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"X_cv\")\n",
    "unigrams_cv=get_unigram_feature_v2(file_ID_cv,OUTPUT_PATH_CV)\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
